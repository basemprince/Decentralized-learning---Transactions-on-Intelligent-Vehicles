\section{Introduction}
%\textcolor{red}{
%\IEEEPARstart{I}{ntelligent} vehicles have the potential to revolutionize transportation by enhancing safety, efficiency, and comfort. However, developing and optimizing \gls*{ml} models for these vehicles requires large amounts of data. Anonymously collecting this data is essential to protecting privacy, while ensuring that the \gls*{ml} models are trained on diverse and representative data to achieve high performance~\cite{comm-efficient}.}

%\textcolor{red}{
%\glspl{gan} are powerful generative models that involve training two neural networks, a generator and a discriminator, to compete in a zero-sum game~\cite{goodfellow_generative_2014}. TimeGAN~\cite{yoon_time-series_2019} is a centralized \gls*{gan} that exploits LSTM~\cite{hochreiter_long_1997} for generating time-series data. It consists of ``embedding'' (encodes data into a latent space) and ``recovery'' (reconstructs data from latent representations) functions that allow to preserve correlations and temporal dynamics of the original data. While centralized \glspl*{gan} have shown promising results in various applications, privacy concerns have led to the development of decentralized \glspl*{gan}, where the model is trained on edge devices without centralizing data~\cite{hardy_md-gan_2019}. In this work, we focus on deploying a decentralized \gls*{gan} model on individual vehicles to generate synthetic data that mimics the behavior of local data.}

%\textcolor{red}{
%The \gls*{niid} nature of the data for each vehicle poses a challenge for training the decentralized \gls*{gan} model effectively. To address this challenge, the \gls*{f2u} approach ~\cite{yonetani_decentralized_2019} trains individual discriminators on local data and updates a generator to fool the most forgiving discriminator that deems generated samples as the most real. They also proposed \gls*{f2a}, which adaptively aggregates discriminators while emphasizing the most forgiving. Similarly, FeGAN~\cite{guerraoui_fegan_2020} fully distributes both the generator and the discriminator so that a private \gls*{gan} can be locally trained on each worker. These approaches help with preventing the vanishing gradients, mode collapse and learning divergence problems in a distributed setup.}
%\textcolor{red}{
%In \cite{ekblom_effgan_2022}, %the authors examined the task of learning a data distribution when training data is decentralized among workers in an \gls*{fl} setup. 
%authors proposed \gls*{effgan} as an ensemble of local expert generators that can learn the data distribution over all workers and mitigate client drift. It is able to train with a large number of local epochs and is more communication efficient than previous methods.}

%\textcolor{red}{
%\gls*{f2u} and \gls*{f2a} are beneficial for the deployment on resource-limited devices, as they necessitate having only the discriminator on the edge devices. Therefore, we adapt both of these architectures to work with time-series data. \gls*{f2u} and \gls*{f2a} train individual discriminators on each vehicle. In~\cite{yonetani_decentralized_2019}, the most forgiving discriminator is chosen to update the global generator. However, we found that the least forgiving discriminator gives a better performance. Moreover, we introduce an adaptive pooling layer in the discriminator to work with sequences of different  length, improving the generator's ability to re-create such sequences.
%
%We also propose a novel normalization technique to further enhance the training process. More in details, the server determines the global minimum and maximum values for each feature, considering the lowest minimum and the highest maximum values received from all the workers. These global boundaries are then sent back to all workers for normalization of their local data. We experimentally show that this approach outperforms the normalization based on the known range of the specific sensors involved.}

%\textcolor{red}{
%Our model can effectively learn from multiple \gls*{niid} time-series and converge to a solution that fools all discriminators. Experimental results show that our approach outperforms existing centralized methods, such as TimeGAN~\cite{yoon_time-series_2019}, for \gls*{niid} time-series in terms of the quality of the generated synthetic data. These findings have significant implications for the development of decentralized \glspl*{gan} in intelligent vehicles and other domains where privacy is a concern.}


\textcolor{blue}{
\IEEEPARstart{I}{ntelligent} vehicles have the potential to revolutionize transportation by enhancing safety, efficiency, and comfort. \marginnote{RC26} However, developing and optimizing \gls*{ml} models for these vehicles requires large amounts of data. A possible solution consists in using synthetic data like images~\cite{Li2023Parallel}. Ideally, synthetic data should be generated automatically by a generative model, like the Generative Adversarial Network (GAN)~\cite{goodfellow_generative_2014}. However, also generative models need to be trained on real to generated synthetic data with high fidelity. Anonymously collecting this data is essential to protecting privacy, while ensuring that the \gls*{ml} models are trained on diverse and representative data to achieve good performance.
Data privacy and \gls*{niid} nature of the data across vehicles pose challenges to this development. In this work, we propose a decentralized \gls*{gan} model that can generate synthetic data, mimicking the behavior of local data, while preserving privacy, i.e.,  without sharing sensitive information across different vehicles.}

We focus on three main contributions:
%
\begin{enumerate}
    \item We present a decentralized \gls*{gan} model that can effectively learn from multiple \gls*{niid} time-series data across vehicles. Our model is trained on edge devices, with individual discriminators on each vehicle. This approach ensures the convergence to a solution that fools all discriminators. Experimental results demonstrate that our method outperforms existing centralized methods, such as TimeGAN, in terms of the quality of the generated synthetic data.
%
    \item We introduce a novel discriminator architecture that uses \glspl*{cnn} and an adaptive pooling layer. This design facilitates the generation of sequences with variable lengths, improving the generator's ability to accurately reproduce such sequences.
%
    \item We propose a unique normalization technique to enhance the training process. This method determines the global minimum and maximum values for each feature considering the lowest minimum and highest maximum values received from all workers. The global boundaries are then disseminated to all workers for normalization of their local data. Experimental evidence shows that this approach outperforms the normalization based on the known range of the specific sensors involved.
\end{enumerate}

These novel contributions have significant implications for the development of decentralized \glspl*{gan} in intelligent vehicles and other domains where privacy is paramount.
%
\section{Related Work}

\glspl*{gan} are powerful models first introduced by Goodfellow et al. These involve training two neural networks, a generator and a discriminator, in a zero-sum game~\cite{goodfellow_generative_2014}. \textcolor{blue}{
\marginnote{RC27} Both the generator and the discriminator are typically based on a Convolutional Neural Network (CNN), a fundamental building block in deep architectures~\cite{lecun2015deep}, that alternates between convolution and pooling layers. Having a pooling layer with fixed stride can limit the performance, therefore, adaptive pooling layers have been proposed for object detection~\cite{tsai2015adaptive}, video annotation~\cite{Liu2017Adaptive}, and feature extraction from near-infrared spectroscopy data~\cite{CHEN2020106303}. Adaptive pooling is also effective in learning large graph neural networks~\cite{ranjan2020asap}. Therefore, in this work, we also exploit adaptive pooling to train GANs on time-series data.
}

In the realm of time-series data, TimeGAN~\cite{yoon_time-series_2019} is a notable development. This model employs \gls*{lstm}~\cite{hochreiter_long_1997} to encode data into a latent space and reconstruct it, thereby preserving the correlations and temporal dynamics of the original data.

Addressing the increasing importance of data privacy, decentralized \glspl*{gan} have emerged. These models train on edge devices without centralizing data~\cite{hardy_md-gan_2019}. Among them, \gls*{f2u}~\cite{yonetani_decentralized_2019} and \gls*{f2a} train individual discriminators on local data and update a generator to fool the most forgiving discriminator. \gls*{f2a} further adaptively aggregates discriminators while emphasizing the most forgiving ones. FeGAN~\cite{guerraoui_fegan_2020} takes a step further by fully distributing both the generator and the discriminator, allowing a private \gls*{gan} to be locally trained on each worker. \gls*{effgan}~\cite{ekblom_effgan_2022} presents an ensemble of local expert generators that can learn the data distribution over all workers and mitigate client drift. It also showcases high communication efficiency and the ability to train with a large number of local epochs.

%textcolor{blue}{
%Recent work has started to bridge the gap between decentralized \glspl*{gan} and time-series data. FeTGAN, proposed by Plesner et al.~\cite{plesner_fetgan_2021}, combines the federated learning framework with TimeGAN to generate realistic time-series data. The model benefits from the throughput and potential data privacy improvements provided by federated learning, while maintaining the ability to reproduce the temporal dynamics of the original data.}

An intersection of Federated Learning, \glspl*{gan}, and autonomous vehicles is explored in the work by Xu et al.~\cite{xu_reliability_2021}, which focuses on the reliability of \gls*{gan} generated data for training and validating perception systems in autonomous vehicles. The authors propose solutions for trusting \gls*{gan}-generated data, particularly for handling corner cases that are difficult to collect large amounts of data for.

Despite these advancements, our work identifies and addresses certain limitations. We adapt the \gls*{f2u} and \gls*{f2a} architectures to work with time-series data, introduce a novel discriminator architecture, and propose a unique normalization technique. We demonstrate that these innovations outperform existing methods in terms of the quality of the generated synthetic data.
