
@misc{brophy_generative_2021,
	title = {Generative adversarial networks in time series: A survey and taxonomy},
	url = {http://arxiv.org/abs/2107.11098},
	shorttitle = {Generative adversarial networks in time series},
	abstract = {Generative adversarial networks ({GANs}) studies have grown exponentially in the past few years. Their impact has been seen mainly in the computer vision field with realistic image and video manipulation, especially generation, making significant advancements. While these computer vision advances have garnered much attention, {GAN} applications have diversified across disciplines such as time series and sequence generation. As a relatively new niche for {GANs}, fieldwork is ongoing to develop high quality, diverse and private time series data. In this paper, we review {GAN} variants designed for time series related applications. We propose a taxonomy of discrete-variant {GANs} and continuous-variant {GANs}, in which {GANs} deal with discrete time series and continuous time series data. Here we showcase the latest and most popular literature in this field; their architectures, results, and applications. We also provide a list of the most popular evaluation metrics and their suitability across applications. Also presented is a discussion of privacy measures for these {GANs} and further protections and directions for dealing with sensitive data. We aim to frame clearly and concisely the latest and state-of-the-art research in this area and their applications to real-world technologies.},
	publisher = {{arXiv}},
	author = {Brophy, Eoin and Wang, Zhengwei and She, Qi and Ward, Tomas},
	urldate = {2022-08-19},
	date = {2021-07-23},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\CLINQ755\\Brophy et al. - 2021 - Generative adversarial networks in time series A .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\47U2RPYT\\2107.html:text/html},
}

@article{yonetani_decentralized_2019,
	title = {Decentralized Learning of Generative Adversarial Networks from Non-iid Data},
	journal = {arXiv/1905.09684},
 author={Ryo Yonetani and Tomohiro Takahashi and Atsushi Hashimoto and Yoshitaka Ushiku},
    year = {2019}
}

@inproceedings{hardy_md-gan_2019,
	title = {{MD}-{GAN}: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets},
	pages = {866--877},
	booktitle = {International Parallel and Distributed Processing Symposium},
	author = {Hardy, Corentin and Merrer, Erwan Le and Sericola, Bruno},
	year = {2019}
}

@inproceedings{guerraoui_fegan_2020,
	title = {{FeGAN}: Scaling Distributed {GANs}},
	booktitle = {Annual {ACM}/{IFIP} Middleware conference},
	author = {Guerraoui, Rachid and Guirguis, Arsany and Kermarrec, Anne-Marie and Le Merrer, Erwan},
        year = {2020}
}

@article{ganesan_synthetic_2021,
	title = {Synthetic time-series data generation using Generative Adversarial Networks},
	url = {https://odr.chalmers.se/handle/20.500.12380/302525},
	abstract = {Data, in this new hybrid era, is the driving force for many industries. The automotive 
industry is no exception to this, and the industry has seen an increasing reliance 
on data for its operations. The automotive sector, nowadays, offers more services 
than just selling a vehicle. It now provides complete mobility solutions like connected, 
shared, and autonomous electric vehicles with personalized options. Such 
services are possible only with the use of high quality data. Manual data collection 
from automobiles is an expensive and laborious task, due to which only a sparse 
amount of high quality data is collected. Reduced data means that the operations 
and analysis performed are also limited, which makes the company able to offer 
less services to the customers. One solution to increase the data is to generate it 
synthetically using deep neural networks. 
Though there are many methods to generate data synthetically, most of them have 
limitations on developing diverse data and preserving the temporal dynamics of the 
original data. This thesis focuses on those issues and studies the possibility of designing 
a neural network model to generate varied time-series data which has the 
characteristics of the original data. In this thesis, a Generative Adversarial Network 
({GAN}) is implemented to synthetically generate time-series data. 
A conventional method of building a machine learning model from scratch is followed 
in this thesis, after weighing several factors. The relevant training data is collected 
from the vehicle and then pre-processed to improve the quality of the data. Following 
this, an initial {GAN} model is developed that contains the generator and 
discriminator structure. Then, an enhanced model with supervised learning mechanism 
called {timeGAN} model is developed for achieving more realistic synthetic 
data. This model is then evaluated with suitable metrics, both in a qualitative and 
quantitative manner. The thesis aims to resolve the issue of scarce data and thus, 
paves the way for effective predictive maintenance of vehicles and better services to 
the customers.},
	author = {Ganesan, Deepak Guru},
	urldate = {2022-08-19},
	date = {2021},
	note = {Accepted: 2021-06-15T09:00:03Z},
	file = {Full Text PDF:C\:\\Users\\bshqga\\Zotero\\storage\\JZWG833J\\Ganesan - 2021 - Synthetic time-series data generation using Genera.pdf:application/pdf;Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\8WLFJR3M\\302525.html:text/html},
}

@book{nord_multivariate_2021,
	title = {Multivariate Time Series Data Generation using Generative Adversarial Networks : Generating Realistic Sensor Time Series Data of Vehicles with an Abnormal Behaviour using {TimeGAN}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-302644},
	shorttitle = {Multivariate Time Series Data Generation using Generative Adversarial Networks},
	abstract = {Large datasets are a crucial requirement to achieve high performance, accuracy, and generalisation for any machine learning task, such as prediction or anomaly detection, However, it is not uncommo ...},
	author = {Nord, Sofia},
	urldate = {2022-08-19},
	date = {2021},
	file = {Full Text PDF:C\:\\Users\\bshqga\\Zotero\\storage\\8B4YKF7T\\Nord - 2021 - Multivariate Time Series Data Generation using Gen.pdf:application/pdf;Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\W7R3TWF8\\record.html:text/html},
}

@article{wiese_quant_2020,
	title = {Quant {GANs}: Deep Generation of Financial Time Series},
	volume = {20},
	issn = {1469-7688, 1469-7696},
	url = {http://arxiv.org/abs/1907.06673},
	doi = {10.1080/14697688.2020.1730426},
	shorttitle = {Quant {GANs}},
	abstract = {Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant {GANs}, a data-driven model which is inspired by the recent success of generative adversarial networks ({GANs}). Quant {GANs} consist of a generator and discriminator function, which utilize temporal convolutional networks ({TCNs}) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant {GANs}, demonstrably in high fidelity.},
	pages = {1419--1440},
	number = {9},
	journaltitle = {Quantitative Finance},
	shortjournal = {Quantitative Finance},
	author = {Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
	urldate = {2022-08-22},
	date = {2020-09-01},
	eprinttype = {arxiv},
	eprint = {1907.06673 [cs, q-fin, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Finance - Computational Finance, Quantitative Finance - Mathematical Finance},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\H9394A9I\\Wiese et al. - 2020 - Quant GANs Deep Generation of Financial Time Seri.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\3KQZX8QI\\1907.html:text/html},
}

@misc{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named {WGAN}, an alternative to traditional {GAN} training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	number = {{arXiv}:1701.07875},
	publisher = {{arXiv}},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, LÃ©on},
	urldate = {2022-08-25},
	date = {2017-12-06},
	eprinttype = {arxiv},
	eprint = {1701.07875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\MLB6KUY6\\Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\5PLFW2FT\\1701.html:text/html},
}

@inproceedings{yoon_time-series_2019,
	title = {Time-series Generative Adversarial Networks},
	volume = {32},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Yoon, Jinsung and Jarrett, Daniel and van der Schaar, Mihaela},
	year = {2019},
}

@online{noauthor_lobstr_nodate,
	title = {{LOBSTR} - Learning On-Board Signals for Timely Reaction {\textbar} Vinnova},
	url = {https://www.vinnova.se/en/p/lobstr---learning-on-board-signals-for-timely-reaction/},
	abstract = {The overall question that the project {LOBSTR} is trying to answer is whether it is possible to apply anomaly detection methods to temporal multivariate signals {\textbar} Vinnova},
	urldate = {2022-08-25},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\8GYFRTNN\\lobstr---learning-on-board-signals-for-timely-reaction.html:text/html},
}

@misc{liu_decentralized_2020,
	title = {A Decentralized Parallel Algorithm for Training Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1910.12999},
	abstract = {Generative Adversarial Networks ({GANs}) are a powerful class of generative models in the deep learning community. Current practice on large-scale {GAN} training utilizes large models and distributed large-batch training strategies, and is implemented on deep learning frameworks (e.g., {TensorFlow}, {PyTorch}, etc.) designed in a centralized manner. In the centralized network topology, every worker needs to either directly communicate with the central node or indirectly communicate with all other workers in every iteration. However, when the network bandwidth is low or network latency is high, the performance would be significantly degraded. Despite recent progress on decentralized algorithms for training deep neural networks, it remains unclear whether it is possible to train {GANs} in a decentralized manner. The main difficulty lies at handling the nonconvex-nonconcave min-max optimization and the decentralized communication simultaneously. In this paper, we address this difficulty by designing the {\textbackslash}textbf\{first gradient-based decentralized parallel algorithm\} which allows workers to have multiple rounds of communications in one iteration and to update the discriminator and generator simultaneously, and this design makes it amenable for the convergence analysis of the proposed decentralized algorithm. Theoretically, our proposed decentralized algorithm is able to solve a class of non-convex non-concave min-max problems with provable non-asymptotic convergence to first-order stationary point. Experimental results on {GANs} demonstrate the effectiveness of the proposed algorithm.},
	number = {{arXiv}:1910.12999},
	publisher = {{arXiv}},
	author = {Liu, Mingrui and Zhang, Wei and Mroueh, Youssef and Cui, Xiaodong and Ross, Jerret and Yang, Tianbao and Das, Payel},
	urldate = {2022-08-26},
	date = {2020-10-19},
	eprinttype = {arxiv},
	eprint = {1910.12999 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\VW2JDRF5\\Liu et al. - 2020 - A Decentralized Parallel Algorithm for Training Ge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\AYTFXYBS\\1910.html:text/html},
}

@article{ekblom_effgan_2022,
	title = {{EFFGAN}: Ensembles of fine-tuned federated {GANs}},
	journal = {arXiv/2206.11682},
 author={Ebba Ekblom and Edvin Listo Zec and Olof Mogren},
    year = {2022}
}

@inproceedings{arnelid_recurrent_2019,
	location = {Auckland, New Zealand},
	title = {Recurrent Conditional Generative Adversarial Networks for Autonomous Driving Sensor Modelling},
	isbn = {978-1-5386-7024-8},
	url = {https://ieeexplore.ieee.org/document/8916999/},
	doi = {10.1109/ITSC.2019.8916999},
	abstract = {Simulation of the real world is a widely researched topic in various ï¬elds. The automotive industry in particular is very dependent on real world simulations, since these simulations are needed in order to prove the safety of advance driver assistance systems ({ADAS}) and autonomous driving ({AD}). In this paper we propose a deep learning based model for simulating the outputs from production sensors used in autonomous vehicles. We introduce an improved Recurrent Conditional Generative Adversarial Network ({RC}-{GAN}) consisting of Recurrent Neural Networks ({RNNs}) that use Long Short-Term Memory ({LSTM}) in both the generator and the discriminator networks in order to generate production sensor errors that exhibit long-term temporal correlations. The network is trained in a sequence-to-sequence fashion where we condition the output from the model on sequences describing the surrounding environment. This enables the model to capture spatial and temporal dependencies, and the model is used to generate synthetic time series describing the errors in a production sensor which can be used for more realistic simulations. The model is trained on a data set collected from real roads with various trafï¬c settings, and yields signiï¬cantly better results as compared to previous works.},
	eventtitle = {2019 {IEEE} Intelligent Transportation Systems Conference - {ITSC}},
	pages = {1613--1618},
	booktitle = {2019 {IEEE} Intelligent Transportation Systems Conference ({ITSC})},
	publisher = {{IEEE}},
	author = {Arnelid, Henrik and Zec, Edvin Listo and Mohammadiha, Nasser},
	urldate = {2022-08-29},
	date = {2019-10},
	langid = {english},
	file = {Arnelid et al. - 2019 - Recurrent Conditional Generative Adversarial Netwo.pdf:C\:\\Users\\bshqga\\Zotero\\storage\\WGECGKJR\\Arnelid et al. - 2019 - Recurrent Conditional Generative Adversarial Netwo.pdf:application/pdf},
}

@misc{zec_specialized_2021,
	title = {Specialized federated learning using a mixture of experts},
	url = {http://arxiv.org/abs/2010.02056},
	abstract = {In federated learning, clients share a global model that has been trained on decentralized local client data. Although federated learning shows significant promise as a key approach when data cannot be shared or centralized, current methods show limited privacy properties and have shortcomings when applied to common real-world scenarios, especially when client data is heterogeneous. In this paper, we propose an alternative method to learn a personalized model for each client in a federated setting, with greater generalization abilities than previous methods. To achieve this personalization we propose a federated learning framework using a mixture of experts to combine the specialist nature of a locally trained model with the generalist knowledge of a global model. We evaluate our method on a variety of datasets with different levels of data heterogeneity, and our results show that the mixture of experts model is better suited as a personalized model for devices in these settings, outperforming both fine-tuned global models and local specialists.},
	number = {{arXiv}:2010.02056},
	publisher = {{arXiv}},
	author = {Zec, Edvin Listo and Mogren, Olof and Martinsson, John and SÃ¼tfeld, Leon RenÃ© and Gillblad, Daniel},
	urldate = {2022-08-29},
	date = {2021-02-08},
	eprinttype = {arxiv},
	eprint = {2010.02056 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\34E3EU8C\\Zec et al. - 2021 - Specialized federated learning using a mixture of .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\TMVBDD2G\\2010.html:text/html},
}

@misc{onoszko_decentralized_2021,
	title = {Decentralized federated learning of deep neural networks on non-iid data},
	url = {http://arxiv.org/abs/2107.08517},
	abstract = {We tackle the non-convex problem of learning a personalized deep learning model in a decentralized setting. More specifically, we study decentralized federated learning, a peer-to-peer setting where data is distributed among many clients and where there is no central server to orchestrate the training. In real world scenarios, the data distributions are often heterogeneous between clients. Therefore, in this work we study the problem of how to efficiently learn a model in a peer-to-peer system with non-iid client data. We propose a method named Performance-Based Neighbor Selection ({PENS}) where clients with similar data distributions detect each other and cooperate by evaluating their training losses on each other's data to learn a model suitable for the local data distribution. Our experiments on benchmark datasets show that our proposed method is able to achieve higher accuracies as compared to strong baselines.},
	number = {{arXiv}:2107.08517},
	publisher = {{arXiv}},
	author = {Onoszko, Noa and Karlsson, Gustav and Mogren, Olof and Zec, Edvin Listo},
	urldate = {2022-08-29},
	date = {2021-07-20},
	eprinttype = {arxiv},
	eprint = {2107.08517 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\M2UW6QE3\\Onoszko et al. - 2021 - Decentralized federated learning of deep neural ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\AMDFWHM2\\2107.html:text/html},
}

@misc{zec_decentralized_2022,
	title = {Decentralized adaptive clustering of deep nets is beneficial for client collaboration},
	url = {http://arxiv.org/abs/2206.08839},
	abstract = {We study the problem of training personalized deep learning models in a decentralized peer-to-peer setting, focusing on the setting where data distributions differ between the clients and where different clients have different local learning tasks. We study both covariate and label shift, and our contribution is an algorithm which for each client finds beneficial collaborations based on a similarity estimate for the local task. Our method does not rely on hyperparameters which are hard to estimate, such as the number of client clusters, but rather continuously adapts to the network topology using soft cluster assignment based on a novel adaptive gossip algorithm. We test the proposed method in various settings where data is not independent and identically distributed among the clients. The experimental evaluation shows that the proposed method performs better than previous state-of-the-art algorithms for this problem setting, and handles situations well where previous methods fail.},
	number = {{arXiv}:2206.08839},
	publisher = {{arXiv}},
	author = {Zec, Edvin Listo and Ekblom, Ebba and Willbo, Martin and Mogren, Olof and Girdzijauskas, Sarunas},
	urldate = {2022-08-29},
	date = {2022-06-17},
	eprinttype = {arxiv},
	eprint = {2206.08839 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\JXWAXE9P\\Zec et al. - 2022 - Decentralized adaptive clustering of deep nets is .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\DURHPYFL\\2206.html:text/html},
}

@misc{odena_conditional_2017,
	title = {Conditional Image Synthesis With Auxiliary Classifier {GANs}},
	url = {http://arxiv.org/abs/1610.09585},
	abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks ({GANs}) for image synthesis. We construct a variant of {GANs} employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 {ImageNet} classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real {ImageNet} data.},
	number = {{arXiv}:1610.09585},
	publisher = {{arXiv}},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	urldate = {2022-08-30},
	date = {2017-07-20},
	eprinttype = {arxiv},
	eprint = {1610.09585 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\2JQ3W7AF\\Odena et al. - 2017 - Conditional Image Synthesis With Auxiliary Classif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\KPUM8GLP\\1610.html:text/html},
}

@inproceedings{goodfellow_generative_2014,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Generative Adversarial Nets},
 volume = {27},
 year = {2014}
}

@misc{ren_new_2021,
	title = {A New Distributed Method for Training Generative Adversarial Networks},
	url = {http://arxiv.org/abs/2107.08681},
	doi = {10.48550/arXiv.2107.08681},
	abstract = {Generative adversarial networks ({GANs}) are emerging machine learning models for generating synthesized data similar to real data by jointly training a generator and a discriminator. In many applications, data and computational resources are distributed over many devices, so centralized computation with all data in one location is infeasible due to privacy and/or communication constraints. This paper proposes a new framework for training {GANs} in a distributed fashion: Each device computes a local discriminator using local data; a single server aggregates their results and computes a global {GAN}. Specifically, in each iteration, the server sends the global {GAN} to the devices, which then update their local discriminators; the devices send their results to the server, which then computes their average as the global discriminator and updates the global generator accordingly. Two different update schedules are designed with different levels of parallelism between the devices and the server. Numerical results obtained using three popular datasets demonstrate that the proposed framework can outperform a state-of-the-art framework in terms of convergence speed.},
	number = {{arXiv}:2107.08681},
	publisher = {{arXiv}},
	author = {Ren, Jinke and Liu, Chonghe and Yu, Guanding and Guo, Dongning},
	urldate = {2022-09-06},
	date = {2021-07-19},
	eprinttype = {arxiv},
	eprint = {2107.08681 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory, Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\ZFCF8ZKM\\Ren et al. - 2021 - A New Distributed Method for Training Generative A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\H858IRSM\\2107.html:text/html},
}

@inproceedings{jin_multi-player_2020,
	location = {London, United Kingdom},
	title = {A Multi-Player Minimax Game for Generative Adversarial Networks},
	isbn = {978-1-72811-331-9},
	url = {https://ieeexplore.ieee.org/document/9102779/},
	doi = {10.1109/ICME46284.2020.9102779},
	eventtitle = {2020 {IEEE} International Conference on Multimedia and Expo ({ICME})},
	pages = {1--6},
	booktitle = {2020 {IEEE} International Conference on Multimedia and Expo ({ICME})},
	publisher = {{IEEE}},
	author = {Jin, Ying and Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Yu, Philip S. and Sun, Jiaguang},
	urldate = {2022-09-12},
	date = {2020-07},
	langid = {english},
	file = {Jin et al. - 2020 - A Multi-Player Minimax Game for Generative Adversa.pdf:C\:\\Users\\bshqga\\Zotero\\storage\\6IFWNUHE\\Jin et al. - 2020 - A Multi-Player Minimax Game for Generative Adversa.pdf:application/pdf},
}

@article{zhang_dance_2022,
	title = {{DANCE}: Distributed Generative Adversarial Networks with Communication Compression},
	volume = {22},
	issn = {1533-5399, 1557-6051},
	url = {https://dl.acm.org/doi/10.1145/3458929},
	doi = {10.1145/3458929},
	shorttitle = {{DANCE}},
	abstract = {Generative adversarial networks ({GANs}) have shown great success in deep representations learning, data generation, and security enhancement. With the development of the Internet of Things, 5th generation wireless systems (5G), and other technologies, the large volume of data collected at the edge of networks provides a new way to improve the capabilities of {GANs}. Due to privacy, bandwidth, and legal constraints, it is not appropriate to upload all the data to the cloud or servers for processing. Therefore, this article focuses on deploying and training {GANs} at the edge rather than converging edge data to the central node. To address this problem, we designed a novel distributed learning architecture for {GANs}, called {DANCE}. {DANCE} can adaptively perform communication compression based on the available bandwidth, while supporting both data and model parallelism training of {GANs}. In addition, inspired by the gossip mechanism and Stackelberg game, a compatible algorithm, {AC}-{GAN} is proposed. The theoretical analysis guarantees the convergence of the model and the existence of approximate equilibrium in {AC}-{GAN}. Both simulation and prototype system experiments show that {AC}-{GAN} can achieve better training effectiveness with less communication overhead than the {SOTA} algorithms, i.e., {FL}-{GAN} and {MD}-{GAN}.},
	pages = {1--32},
	number = {2},
	journaltitle = {{ACM} Transactions on Internet Technology},
	shortjournal = {{ACM} Trans. Internet Technol.},
	author = {Zhang, Xiongtao and Zhu, Xiaomin and Wang, Ji and Bao, Weidong and Yang, Laurence T.},
	urldate = {2022-09-12},
	date = {2022-05-31},
	langid = {english},
	file = {Zhang et al. - 2022 - DANCE Distributed Generative Adversarial Networks.pdf:C\:\\Users\\bshqga\\Zotero\\storage\\QJUMHACK\\Zhang et al. - 2022 - DANCE Distributed Generative Adversarial Networks.pdf:application/pdf},
}

@misc{durugkar_generative_2017,
	title = {Generative Multi-Adversarial Networks},
	url = {http://arxiv.org/abs/1611.01673},
	abstract = {Generative adversarial networks ({GANs}) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the {\textbackslash}emph\{Generative Multi-Adversarial Network\} ({GMAN}), a framework that extends {GANs} to multiple discriminators. In previous work, the successful training of {GANs} requires modifying the minimax objective to accelerate training early on. In contrast, {GMAN} can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard {GANs} demonstrate {GMAN} produces higher quality samples in a fraction of the iterations when measured by a pairwise {GAM}-type metric.},
	number = {{arXiv}:1611.01673},
	publisher = {{arXiv}},
	author = {Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar},
	urldate = {2022-09-12},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1611.01673 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\HGC6F7LA\\Durugkar et al. - 2017 - Generative Multi-Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\Z48VRDBQ\\1611.html:text/html},
}

@InProceedings{miyato_spectral_2018,
  title={Spectral Normalization for Generative Adversarial Networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  booktitle={International Conference on Learning Representations ({ICLR})},
  year={2018}
}

@InProceedings{adam_optimizer,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, D.P. and Ba, J.},
  booktitle={International Conference on Learning Representations ({ICLR})},
  year={2015}
}

@misc{mao_least_2017,
	title = {Least Squares Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1611.04076},
	abstract = {Unsupervised learning with generative adversarial networks ({GANs}) has proven hugely successful. Regular {GANs} hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks ({LSGANs}) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of {LSGAN} yields minimizing the Pearson \${\textbackslash}chi{\textasciicircum}2\$ divergence. There are two benefits of {LSGANs} over regular {GANs}. First, {LSGANs} are able to generate higher quality images than regular {GANs}. Second, {LSGANs} perform more stable during the learning process. We evaluate {LSGANs} on five scene datasets and the experimental results show that the images generated by {LSGANs} are of better quality than the ones generated by regular {GANs}. We also conduct two comparison experiments between {LSGANs} and regular {GANs} to illustrate the stability of {LSGANs}.},
	number = {{arXiv}:1611.04076},
	publisher = {{arXiv}},
	author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
	urldate = {2022-09-29},
	date = {2017-04-05},
	eprinttype = {arxiv},
	eprint = {1611.04076 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\HPTNC6GX\\Mao et al. - 2017 - Least Squares Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\S4PJ3CEV\\1611.html:text/html},
}

@article{plebe_reliable_2020,
	title = {On Reliable Neural Network Sensorimotor Control in Autonomous Vehicles},
	volume = {21},
	issn = {1558-0016},
	doi = {10.1109/TITS.2019.2896375},
	abstract = {This paper deals with (deep) neural network implementations of sensorimotor control for automated driving. We show how to construct complex behaviors by re-using elementary neural network building blocks that can be trained and tested extensively; one of our goals is to mitigate the âblack boxâ and verifiability issues that affect end-to-end trained networks. By structuring complex behaviors within a subsumption architecture, we retain the ability to learn (mostly at motor primitives level) with the ability to create complex behaviors by subsuming the (well-known) learned elementary perception-action loops. The learning process itself is simplified, since the agent needs only to learn elementary behaviors. At the same time, the structure imposed with the subsumption architecture ensures that the agent behaves in predictable ways (e.g., treating all obstacles uniformly). We demonstrate these ideas for longitudinal obstacle avoidance behavior, but the proposed approach can also be adapted to other situations.},
	pages = {711--722},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Transportation Systems},
	author = {Plebe, Alice and Da Lio, Mauro and Bortoluzzi, Daniele},
	date = {2020-02},
	note = {Conference Name: {IEEE} Transactions on Intelligent Transportation Systems},
	keywords = {Automobiles, Autonomous vehicles, channel coding, Computer architecture, {ISO} Standards, layered control, longitudinal control, neural networks, Neural networks, Reliability, Safety, subsumption architecture},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bshqga\\Zotero\\storage\\XCD6HRP4\\8643729.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bshqga\\Zotero\\storage\\4JJZ7UT9\\Plebe et al. - 2020 - On Reliable Neural Network Sensorimotor Control in.pdf:application/pdf},
}

@article{rybalkin_efficient_2020,
	title = {Efficient Hardware Architectures for 1D- and {MD}-{LSTM} Networks},
	volume = {92},
	issn = {1939-8115},
	url = {https://doi.org/10.1007/s11265-020-01554-x},
	doi = {10.1007/s11265-020-01554-x},
	abstract = {Recurrent Neural Networks, in particular One-dimensional and Multidimensional Long Short-Term Memory (1D-{LSTM} and {MD}-{LSTM}) have achieved state-of-the-art classification accuracy in many applications such as machine translation, image caption generation, handwritten text recognition, medical imaging and many more. However, high classification accuracy comes at high compute, storage, and memory bandwidth requirements, which make their deployment challenging, especially for energy-constrained platforms such as portable devices. In comparison to {CNNs}, not so many investigations exist on efficient hardware implementations for 1D-{LSTM} especially under energy constraints, and there is no research publication on hardware architecture for {MD}-{LSTM}. In this article, we present two novel architectures for {LSTM} inference: a hardware architecture for {MD}-{LSTM}, and a {DRAM}-based Processing-in-Memory ({DRAM}-{PIM}) hardware architecture for 1D-{LSTM}. We present for the first time a hardware architecture for {MD}-{LSTM}, and show a trade-off analysis for accuracy and hardware cost for various precisions. We implement the new architecture as an {FPGA}-based accelerator that outperforms {NVIDIA} K80 {GPU} implementation in terms of runtime by up to 84Ã and energy efficiency by up to 1238Ã for a challenging dataset for historical document image binarization from {DIBCO} 2017 contest, and a well known {MNIST} dataset for handwritten digits recognition. Our accelerator demonstrates highest accuracy and comparable throughput in comparison to state-of-the-art {FPGA}-based implementations of multilayer perceptron for {MNIST} dataset. Furthermore, we present a new {DRAM}-{PIM} architecture for 1D-{LSTM} targeting energy efficient compute platforms such as portable devices. The {DRAM}-{PIM} architecture integrates the computation units in a close proximity to the {DRAM} cells in order to maximize the data parallelism and energy efficiency. The proposed {DRAM}-{PIM} design is 16.19 Ã more energy efficient as compared to {FPGA} implementation. The total chip area overhead of this design is 18 \% compared to a commodity 8 Gb {DRAM} chip. Our experiments show that the {DRAM}-{PIM} implementation delivers a throughput of 1309.16 {GOp}/s for an optical character recognition application.},
	pages = {1219--1245},
	number = {11},
	journaltitle = {Journal of Signal Processing Systems},
	shortjournal = {Journal of Signal Processing Systems},
	author = {Rybalkin, Vladimir and Sudarshan, Chirag and Weis, Christian and Lappas, Jan and Wehn, Norbert and Cheng, Li},
	date = {2020-11-01},
}

@article{rybalkin_efficient_2020-1,
	title = {Efficient Hardware Architectures for 1D- and {MD}-{LSTM} Networks},
	volume = {92},
	issn = {1939-8115},
	url = {https://doi.org/10.1007/s11265-020-01554-x},
	doi = {10.1007/s11265-020-01554-x},
	abstract = {Recurrent Neural Networks, in particular One-dimensional and Multidimensional Long Short-Term Memory (1D-{LSTM} and {MD}-{LSTM}) have achieved state-of-the-art classification accuracy in many applications such as machine translation, image caption generation, handwritten text recognition, medical imaging and many more. However, high classification accuracy comes at high compute, storage, and memory bandwidth requirements, which make their deployment challenging, especially for energy-constrained platforms such as portable devices. In comparison to {CNNs}, not so many investigations exist on efficient hardware implementations for 1D-{LSTM} especially under energy constraints, and there is no research publication on hardware architecture for {MD}-{LSTM}. In this article, we present two novel architectures for {LSTM} inference: a hardware architecture for {MD}-{LSTM}, and a {DRAM}-based Processing-in-Memory ({DRAM}-{PIM}) hardware architecture for 1D-{LSTM}. We present for the first time a hardware architecture for {MD}-{LSTM}, and show a trade-off analysis for accuracy and hardware cost for various precisions. We implement the new architecture as an {FPGA}-based accelerator that outperforms {NVIDIA} K80 {GPU} implementation in terms of runtime by up to 84Ã and energy efficiency by up to 1238Ã for a challenging dataset for historical document image binarization from {DIBCO} 2017 contest, and a well known {MNIST} dataset for handwritten digits recognition. Our accelerator demonstrates highest accuracy and comparable throughput in comparison to state-of-the-art {FPGA}-based implementations of multilayer perceptron for {MNIST} dataset. Furthermore, we present a new {DRAM}-{PIM} architecture for 1D-{LSTM} targeting energy efficient compute platforms such as portable devices. The {DRAM}-{PIM} architecture integrates the computation units in a close proximity to the {DRAM} cells in order to maximize the data parallelism and energy efficiency. The proposed {DRAM}-{PIM} design is 16.19 Ã more energy efficient as compared to {FPGA} implementation. The total chip area overhead of this design is 18 \% compared to a commodity 8 Gb {DRAM} chip. Our experiments show that the {DRAM}-{PIM} implementation delivers a throughput of 1309.16 {GOp}/s for an optical character recognition application.},
	pages = {1219--1245},
	number = {11},
	journaltitle = {Journal of Signal Processing Systems},
	shortjournal = {J Sign Process Syst},
	author = {Rybalkin, Vladimir and Sudarshan, Chirag and Weis, Christian and Lappas, Jan and Wehn, Norbert and Cheng, Li},
	urldate = {2022-12-12},
	date = {2020-11-01},
	langid = {english},
	keywords = {2D-{LSTM}, Deep learning, {DIBCO}, {DRAM}, {FPGA}, Hardware architecture, Image binarization, Long short-term memory, {LSTM}, {MD}-{LSTM}, {MNIST}, {OCR}, Optical character recognition, {PIM}, Processing-in-memory, Zynq},
	file = {Full Text PDF:C\:\\Users\\bshqga\\Zotero\\storage\\EHB4QS8V\\Rybalkin et al. - 2020 - Efficient Hardware Architectures for 1D- and MD-LS.pdf:application/pdf},
}

@misc{nakkiran_deep_2019,
	title = {Deep Double Descent: Where Bigger Models and More Data Hurt},
	url = {http://arxiv.org/abs/1912.02292},
	doi = {10.48550/arXiv.1912.02292},
	shorttitle = {Deep Double Descent},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	number = {{arXiv}:1912.02292},
	publisher = {{arXiv}},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	urldate = {2022-12-13},
	date = {2019-12-04},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\KVGMPKN4\\Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\W3Y7EIK3\\1912.html:text/html},
}

@online{noauthor_understanding_nodate,
	title = {Understanding principal components analysis ({PCA})},
	url = {https://www.neuraldesigner.com/blog/principal-components-analysis},
	urldate = {2022-10-16},
	file = {Understanding principal components analysis (PCA) | Neural Designer:C\:\\Users\\bshqga\\Zotero\\storage\\QLDGHECB\\principal-components-analysis.html:text/html},
}

@online{weng_what_2021,
	title = {What are Diffusion Models?},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	abstract = {[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, {GLIDE}, {unCLIP} and Imagen. [Updated on 2022-08-31: Added latent diffusion model.
So far, Iâve written about three types of generative models, {GAN}, {VAE}, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.},
	author = {Weng, Lilian},
	urldate = {2022-11-19},
	date = {2021-07-11},
	langid = {english},
	note = {Section: posts},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\92V379JW\\2021-07-11-diffusion-models.html:text/html},
}

@misc{gupta_model_2016,
	title = {Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A Systematic Study},
	url = {http://arxiv.org/abs/1509.04210},
	shorttitle = {Model Accuracy and Runtime Tradeoff in Distributed Deep Learning},
	abstract = {This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks. Using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtime performance and model accuracy. We introduce a new learning rate modulation strategy to counter the effect of stale gradients and propose a new synchronization protocol that can effectively bound the staleness in gradients, improve runtime performance and achieve good model accuracy. Our empirical investigation reveals a principled approach for distributed training of neural networks: the mini-batch size per learner should be reduced as more learners are added to the system to preserve the model accuracy. We validate this approach using commonly-used image classification benchmarks: {CIFAR}10 and {ImageNet}.},
	number = {{arXiv}:1509.04210},
	publisher = {{arXiv}},
	author = {Gupta, Suyog and Zhang, Wei and Wang, Fei},
	urldate = {2022-12-23},
	date = {2016-12-05},
	eprinttype = {arxiv},
	eprint = {1509.04210 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\86E5XPEH\\Gupta et al. - 2016 - Model Accuracy and Runtime Tradeoff in Distributed.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\YR9E4R66\\1509.html:text/html},
}

@online{anwar_what_2021,
	title = {What is Transposed Convolutional Layer?},
	url = {https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11},
	abstract = {Explained through animated gifs and python code.},
	titleaddon = {Medium},
	author = {Anwar, Aqeel},
	urldate = {2022-10-25},
	date = {2021-04-16},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\2AS52F9Q\\what-is-transposed-convolutional-layer-40e5e6e31c11.html:text/html},
}

@online{leung_dying_2021,
	title = {The Dying {ReLU} Problem, Clearly Explained},
	url = {https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24},
	abstract = {Keep your neural network alive by understanding the downsides of {ReLU}},
	titleaddon = {Medium},
	author = {Leung, Kenneth},
	urldate = {2022-12-27},
	date = {2021-09-23},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\YW58EAX2\\the-dying-relu-problem-clearly-explained-42d0c54e0d24.html:text/html},
}

@misc{li_tts-gan_2022,
	title = {{TTS}-{GAN}: A Transformer-based Time-Series Generative Adversarial Network},
	url = {http://arxiv.org/abs/2202.02691},
	shorttitle = {{TTS}-{GAN}},
	abstract = {Signal measurements appearing in the form of time series are one of the most common types of data used in medical machine learning applications. However, such datasets are often small, making the training of deep neural network architectures ineffective. For time-series, the suite of data augmentation tricks we can use to expand the size of the dataset is limited by the need to maintain the basic properties of the signal. Data generated by a Generative Adversarial Network ({GAN}) can be utilized as another data augmentation tool. {RNN}-based {GANs} suffer from the fact that they cannot effectively model long sequences of data points with irregular temporal relations. To tackle these problems, we introduce {TTS}-{GAN}, a transformer-based {GAN} which can successfully generate realistic synthetic time-series data sequences of arbitrary length, similar to the real ones. Both the generator and discriminator networks of the {GAN} model are built using a pure transformer encoder architecture. We use visualizations and dimensionality reduction techniques to demonstrate the similarity of real and generated time-series data. We also compare the quality of our generated data with the best existing alternative, which is an {RNN}-based time-series {GAN}.},
	number = {{arXiv}:2202.02691},
	publisher = {{arXiv}},
	author = {Li, Xiaomin and Metsis, Vangelis and Wang, Huangyingrui and Ngu, Anne Hee Hiong},
	urldate = {2022-12-28},
	date = {2022-06-26},
	eprinttype = {arxiv},
	eprint = {2202.02691 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\GP627RUS\\Li et al. - 2022 - TTS-GAN A Transformer-based Time-Series Generativ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\CJ7Q2YZS\\2202.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2022-09-24},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	note = {version: 9},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\BINBRJRX\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\LNWX42XS\\1412.html:text/html},
}

@online{antoniadis_activation_2022,
	title = {Activation Functions: Sigmoid vs Tanh},
	url = {https://www.baeldung.com/cs/sigmoid-vs-tanh-functions},
	shorttitle = {Activation Functions},
	abstract = {Explore two activation functions, the tanh and the sigmoid.},
	author = {Antoniadis, Panagiotis},
	urldate = {2022-12-29},
	date = {2022-02-28},
	langid = {american},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\7ZPMJJPK\\sigmoid-vs-tanh-functions.html:text/html},
}

@misc{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	number = {{arXiv}:1502.03167},
	publisher = {{arXiv}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2022-10-29},
	date = {2015-03-02},
	eprinttype = {arxiv},
	eprint = {1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\BJG7Z46R\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\TMGI6HJK\\1502.html:text/html},
}

@online{hui_gan_2021,
	title = {{GAN} â Spectral Normalization},
	url = {https://jonathan-hui.medium.com/gan-spectral-normalization-893b6a4e8f53},
	abstract = {{GAN} is vulnerable to mode collapse and training instability. In this article, we look into {WGAN} and Spectral Normalization in resolvingâ¦},
	titleaddon = {Medium},
	author = {Hui, Jonathan},
	urldate = {2022-09-26},
	date = {2021-08-20},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\5Q4Z8Q6R\\gan-spectral-normalization-893b6a4e8f53.html:text/html},
}

@online{nam_time_2020,
	title = {Time series data characteristics},
	url = {https://medium.com/@namnguyenthe/time-series-data-characteristics-994e43c470c6},
	abstract = {3 as-old-as-the-hills characteristics of a time series any data scientist must know},
	titleaddon = {Medium},
	author = {Nam, The},
	urldate = {2022-12-30},
	date = {2020-10-07},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\TBB5QIBU\\time-series-data-characteristics-994e43c470c6.html:text/html},
}

@online{kumar_why_2021,
	title = {Why is Odd sized kernel preferred over Even sized kernel?},
	url = {https://medium.com/geekculture/why-is-odd-sized-kernel-preferred-over-even-sized-kernel-a767e47b1d77},
	abstract = {I hope everyone will be aware of the role of kernels in Computer Vision. You must have observed that We generally use Odd sizes of Kernelâ¦},
	titleaddon = {Geek Culture},
	author = {Kumar, Prasant},
	urldate = {2023-01-02},
	date = {2021-08-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\GIGCUXJI\\why-is-odd-sized-kernel-preferred-over-even-sized-kernel-a767e47b1d77.html:text/html},
}

@online{noauthor_federated_nodate,
	title = {Federated Learning: The Shift from Centralized to Distributed On-Device Model Training},
	url = {https://www.altexsoft.com/blog/federated-learning/},
	shorttitle = {Federated Learning},
	abstract = {There has been a lot of buzz around data science, machine learning ({ML}), and artificial intelligence ({AI}) lately. As you may already know, training a machine le},
	titleaddon = {{AltexSoft}},
	urldate = {2023-01-02},
	langid = {american},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\RT58E2DE\\federated-learning.html:text/html},
}

@online{noauthor_federated_nodate-1,
	title = {Federated Learning: Collaborative Machine Learning without Centralized Training Data},
	url = {https://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	shorttitle = {Federated Learning},
	urldate = {2023-01-02},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\XMB5WSST\\federated-learning-collaborative.html:text/html},
}

@online{bento_multilayer_2021,
	title = {Multilayer Perceptron Explained with a Real-Life Example and Python Code: Sentiment Analysis},
	url = {https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141},
	shorttitle = {Multilayer Perceptron Explained with a Real-Life Example and Python Code},
	abstract = {Multilayer Perceptron is a Neural Network that learns the relationship between linear and non-linear data.},
	titleaddon = {Medium},
	author = {Bento, Carolina},
	urldate = {2023-01-02},
	date = {2021-09-30},
	langid = {english},
}

@online{saha_comprehensive_2022,
	title = {A Comprehensive Guide to Convolutional Neural Networks},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machinesâ¦},
	titleaddon = {Medium},
	author = {Saha, Sumit},
	urldate = {2023-01-02},
	date = {2022-11-16},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\YFBFNJ7G\\a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.html:text/html},
}

@online{dinesh_cnn_2019,
	title = {{CNN} vs {MLP} for Image Classification},
	url = {https://medium.com/analytics-vidhya/cnn-convolutional-neural-network-8d0a292b4498},
	abstract = {Why {CNN} is preferred over {ANN} for image classification?},
	titleaddon = {Analytics Vidhya},
	author = {Dinesh},
	urldate = {2023-01-02},
	date = {2019-11-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\FESBGZGZ\\cnn-convolutional-neural-network-8d0a292b4498.html:text/html},
}

@online{noauthor_cs_nodate,
	title = {{CS} 230 - Recurrent Neural Networks Cheatsheet},
	url = {https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks},
	urldate = {2023-01-02},
	file = {CS 230 - Recurrent Neural Networks Cheatsheet:C\:\\Users\\bshqga\\Zotero\\storage\\45UUP3AT\\cheatsheet-recurrent-neural-networks.html:text/html},
}

@online{noauthor_understanding_nodate-1,
	title = {Understanding {LSTM} Networks -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2023-01-02},
	file = {Understanding LSTM Networks -- colah's blog:C\:\\Users\\bshqga\\Zotero\\storage\\H2KPIBWD\\2015-08-Understanding-LSTMs.html:text/html},
}

@online{rocca_understanding_2021,
	title = {Understanding Variational Autoencoders ({VAEs})},
	url = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
	abstract = {Building, step by step, the reasoning that leads to {VAEs}.},
	titleaddon = {Medium},
	author = {Rocca, Joseph},
	urldate = {2023-01-02},
	date = {2021-03-21},
	langid = {english}
}

@online{traindatahub_interpretation_2022,
	title = {Interpretation of Evaluation Metrics For Regression Analysis ({MAE}, {MSE}, {RMSE}, {MAPE}, R-Squared, Andâ¦},
	url = {https://medium.com/@ooemma83/interpretation-of-evaluation-metrics-for-regression-analysis-mae-mse-rmse-mape-r-squared-and-5693b61a9833},
	abstract = {In regression analysis, {MAE}, {MSE}, {RMSE}, R squared, and Adjusted R2 metrics are mainly used metrics to evaluate the performance of theâ¦},
	titleaddon = {Medium},
	author = {{TrainDataHub}},
	urldate = {2023-01-08},
	date = {2022-05-23},
	langid = {english},
}

@online{alake_deep_2020,
	title = {Deep Learning: Understand The Inception Module},
	url = {https://towardsdatascience.com/deep-learning-understand-the-inception-module-56146866e652},
	shorttitle = {Deep Learning},
	abstract = {The Deep Learning Architecture Inspired By An Internet Memeâââand its technical information and details},
	titleaddon = {Medium},
	author = {Alake, Richmond},
	urldate = {2023-01-08},
	date = {2020-12-22},
	langid = {english},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\3XB6TB56\\deep-learning-understand-the-inception-module-56146866e652.html:text/html},
}

@online{noauthor_softmax_2019,
	title = {Softmax Function},
	url = {https://deepai.org/machine-learning-glossary-and-terms/softmax-layer},
	abstract = {The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities.},
	titleaddon = {{DeepAI}},
	urldate = {2023-01-08},
	date = {2019-05-17},
	file = {Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\CRIMR4N2\\softmax-layer.html:text/html},
}

@misc{heusel_gans_2018,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	url = {http://arxiv.org/abs/1706.08500},
	doi = {10.48550/arXiv.1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	number = {{arXiv}:1706.08500},
	publisher = {{arXiv}},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2023-01-08},
	date = {2018-01-12},
	eprinttype = {arxiv},
	eprint = {1706.08500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\AUYQFWU3\\Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\ZB8NAUDZ\\1706.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-01-09},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\24KGEK8J\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\3TR92YX3\\1706.html:text/html},
}

@misc{tuli_tranad_2022,
	title = {{TranAD}: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data},
	url = {http://arxiv.org/abs/2201.07284},
	shorttitle = {{TranAD}},
	abstract = {Efficient anomaly detection and diagnosis in multivariate time-series data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose {TranAD}, a deep transformer network based anomaly detection and diagnosis model which uses attention-based sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. {TranAD} uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability. Additionally, model-agnostic meta learning ({MAML}) allows us to train the model using limited data. Extensive empirical studies on six publicly available datasets demonstrate that {TranAD} can outperform state-of-the-art baseline methods in detection and diagnosis performance with data and time-efficient training. Specifically, {TranAD} increases F1 scores by up to 17\%, reducing training times by up to 99\% compared to the baselines.},
	number = {{arXiv}:2201.07284},
	publisher = {{arXiv}},
	author = {Tuli, Shreshth and Casale, Giuliano and Jennings, Nicholas R.},
	urldate = {2023-01-10},
	date = {2022-05-14},
	eprinttype = {arxiv},
	eprint = {2201.07284 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\bshqga\\Zotero\\storage\\JPD2SRTI\\Tuli et al. - 2022 - TranAD Deep Transformer Networks for Anomaly Dete.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bshqga\\Zotero\\storage\\7QNDMY8K\\2201.html:text/html},
}

@article{alcaraz_diffusion-based_2022,
	title = {Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models},
	author = {Alcaraz, Juan Miguel Lopez and Strodthoff, Nils},
	journal={Transactions on Machine Learning Research},
    year={2022}
}

@incollection{he_spatial_2014,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {8691},
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2014},
	pages = {346--361},
	booktitle = {Advances in Neural Information Processing Systems}
}

@misc{ye_three_2021,
	title = {Three {Model} {Compression} {Methods} {You} {Need} {To} {Know} in 2021},
	url = {https://tinyurl.com/bdfzp4m7},
	abstract = {Creative techniques to make complex models smaller},
	language = {en},
	urldate = {2023-01-04},
	journal = {Medium},
	author = {Ye, Andre},
	year = {2021}
}

@InProceedings{comm-efficient,
  title = {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
  author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1273--1282},
  year = 	 {2017},
  volume = 	 {54}
}



@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{Ismail_Fawaz_2019,
	year = 2019,
	volume = {33},
	number = {4},
	pages = {917--963},
	author = {Hassan Ismail Fawaz and Germain Forestier and Jonathan Weber and Lhassane Idoumghar and Pierre-Alain Muller},
	title = {Deep learning for time series classification: a review},
	journal = {Data Mining and Knowledge Discovery}
}

@ARTICLE{rnntimeseries,
  author={Connor, J.T. and Martin, R.D. and Atlas, L.E.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Recurrent neural networks and robust time series prediction}, 
  year={1994},
  volume={5},
  number={2},
  pages={240-254},
  doi={10.1109/72.279188}}


@article{rasul2021autoregressive,
      title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting}, 
      author={Kashif Rasul and Calvin Seward and Ingmar Schuster and Roland Vollgraf},
      year={2021},
      journal={arXiv/2101.12072}
}

@article{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      journal={arXiv/1503.02531}
}

@INPROCEEDINGS{xu_reliability_2021,
  author={Xu, Weihuang and Souly, Nasim and Brahma, Pratik Prabhanjan},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)}, 
  title={Reliability of GAN Generated Data to Train and Validate Perception Systems for Autonomous Vehicles}, 
  year={2021},
  volume={},
  number={},
  pages={171-180},
  doi={10.1109/WACVW52041.2021.00023}}

@article{plesner_fetgan_2021,
    title = {{FeTGAN}: {Federated} {Time}-{Series} {Generative} {Adversarial} {Network}},
    shorttitle = {{FeTGAN}},
    url = {https://repository.tudelft.nl/islandora/object/uuid%3A5e79a3d4-4ba1-4baa-af1d-c744b545e3df},
    journal = {Unknown},
    abstract = {The key to producing high-fidelity time-series data...},
    language = {en},
    urldate = {2023-05-28},
    author = {Plesner, M. K.},
    year = {2021},
}

@misc{esteban2017realvalued,
      title={Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs}, 
      author={CristÃ³bal Esteban and Stephanie L. Hyland and Gunnar RÃ¤tsch},
      year={2017},
      eprint={1706.02633},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}