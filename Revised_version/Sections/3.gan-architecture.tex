\section{The Generative Adversarial Network Architecture}
%We can use similar sub-sections of the thesis. All this section will be 1/3 pages.

In our approach, we adopted \gls*{f2u} to learn from \gls*{niid} data. In the original architecture, designed for image data, the layers used 2D convolution and transpose convolution, with the width and height dimensions representing the spatial dimensions of the images. 
%
The original \gls*{f2u} uses \glspl*{cnn} for both the generator and discriminator networks. The generator supports five layers of 2D transpose convolution with batch normalization and \gls*{relu} activation functions. On the other hand, the discriminator includes five layers of 2D convolution with spectral normalization \cite{miyato_spectral_2018} and Leaky \gls*{relu}, then later a \gls*{fc} layer. It should be noted that the input size to the \gls*{fc} layer must be initialized based on the input size. That means changing the input size to the discriminator is unfeasible in the current setup.
In the rest of this section, we show how to adapt \gls*{f2u} to learn from time-series data.


\subsection{Time-series Architecture Adaptation}

The first step to adapt the architecture for time-series data is to set up the convolutional layers. This can be done using one of the following structures (see Fig.~\ref{fig:conv-options}):
%
\begin{enumerate}
    %\item \textcolor{red}{1D convolution and transpose convolution, with each channel of the 1D convolution layers representing a specific feature and the width dimension representing the time domain. With $M$ features and a time window of $T$ seconds, a convolution layer would have $M$ channels and a 1D convolution width of $T$.}
%    \item[2)] 1D convolution and transpose convolution, with each channel in the time domain and the width representing a separate feature. With $M$ features and a time window of $T$ seconds, a convolution layer would have $T$ channels and a 1D convolution width of $M$.
    \item \textcolor{blue}{\marginnote{RC14} 1D convolution and transpose convolution can be organized in two ways: First, each channel can represent a specific feature, with the width dimension symbolizing the time domain, resulting in $M$ channels and a 1D convolution width of $T$ seconds. Alternatively, each channel can exist in the time domain, with the width representing a different feature, leading to $T$ channels and a 1D convolution width of $M$ features.}
    \item 2D convolution and transpose convolution, with the width axis in time domain and the height axis in feature domain. Each channel would be a copy of the previous one, except the first row is moved to the bottom.
\end{enumerate}
%
We use 1) (top-left structure in Fig.~\ref{fig:conv-options}) in our approach. The reason is that applying convolutions along the time dimension allows the network to learn patterns across different time points in the data. For a given feature, there exists more dependency on the previous time step of that feature compared to the other features \cite{Ismail_Fawaz_2019}.
%
While the use of 2D convolution with feature mirroring might improve the network's ability to identify feature dependencies, it would also significantly complicate the network and increase its size. Furthermore, implementing 2D convolution would require modifying the output from the generator to match the new 2D convolution setup. For these reasons, we did not pursue a 2D convolution in this paper.
%
We switched both the generator and discriminator to 1D convolution as previously mentioned and further modified the sizes of the kernel, stride, and output channels to fit the feature count and sequence width of the time-series data. 
% \atB{How we modify these}

%\input{Figures/conv-options-old.tex}
\input{Figures/conv-options.tex}


\subsection{Time-series Based Neural Network Architecture}
%Some notes on the "Image Based Neural Network Architecture".
The proposed generator and discriminator are shown in Fig.~\ref{fig:gan-arch-time}. In this paper, we explored two sliding window approaches for creating training samples. The first uses a fixed-length window of with size $t$ and step size $s$ to divide the
original time series into $N$ sub-sequences. The second approach employs a variable-length (dynamic) window. Using a dynamic window when training a \gls*{gan} may enhance the generator's ability to create sequences of varying lengths, as opposed to a fixed-size window that restricts the generator to producing sequences of a specific length.
%
Implementing a dynamic window is a challenge considering that the discriminator is \gls*{cnn}-based with a \gls*{fc} layer. Once such a network is initialized, changing the input dimensions during training can cause problems with the internal structure and weights of the network. Nonetheless, we employed the use of adaptive average pooling after the convolution layers to make the network work with inputs of different sizes~\cite{he_spatial_2014}. 

The dynamic window implementation is simpler for the generator as it only requires changing the dimension of the noise input.  In order to control the output sequence width, we used Alg.~\ref{alg:seq-find} to calculate the size of the required noise input. When provided with the target sequence width and the generator parameters (kernel sizes, paddings, and strides), this algorithm approximates the noise size by working backwards through the layers calculating the input size of each layer. The noise size is then rounded up and used to calculate the actual output size by working forwards through the layers. Note that the output size may not be exactly equal to the target sequence width due to the fixed sizes of the generator parameters. 

\input{Algorithms/seq-find.tex}

We further use the calculated actual sequence width from the generator to divide and create samples out of the real time-series data. That means both the real and synthetic samples would have the same dimensions.  
%
We selected an initial sequence width to initialize the discriminator. This involves calculating the number of neurons after flattening the output from the last convolution layers using the backward loop in Alg.~\ref{alg:seq-find}. The calculated number of neurons is then used to initialize the expected input size for the linear layer. Furthermore, it also specifies the required output dimension for the adaptive average pooling layer to match the input size of that linear layer. Following steps outline the training sequence:
%
\begin{itemize}
\item Create a list of increasing sequence lengths [e.g., 64, 128, 192, 256, and 320 seconds]. 
% \atB{Why these and not more?}
\item Calculate the required noise size and actual sequence length for each of the sequence lengths using Alg.~\ref{alg:seq-find} [e.g., 65, 129, 193, 257, and 321 seconds].
\item Use the shortest sequence length to initialize discriminator and specify input size for the linear layer. Choosing the shortest sequence guarantees that the average pooling layer would always be down-sampling.
%
\item To ensure that the neural network sees the full data-set at every epoch while maintaining a constant batch size, it is important to keep the sequence length constant during the iterations of each epoch. Both real and synthetic data are sampled to match this sequence length. As a result, the iteration count will vary across epochs.
%
\item During the sequence length change, the output from the last convolutional layer in the discriminator changes. The adaptive average pooling layer makes sure to change that dimension to the initialized dimension for the linear layer.
\end{itemize}
%
The proposed \gls*{f2u} architecture is capable of processing time-series data and of handling sequence of varying length both for training and generation.

\input{Figures/gan-arch.tex}