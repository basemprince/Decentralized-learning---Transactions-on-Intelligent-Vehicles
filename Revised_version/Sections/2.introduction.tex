\section{Introduction}

\IEEEPARstart{I}{ntelligent} vehicles have the potential to revolutionize transportation by enhancing safety, efficiency, and comfort. However, developing and optimizing \gls*{ml} models for these vehicles requires large amounts of data. Anonymously collecting this data is essential to protecting privacy, while ensuring that the \gls*{ml} models are trained on diverse and representative data to achieve high performance~\cite{comm-efficient}.

\glspl*{gan} are powerful generative models that involve training two neural networks, a generator and a discriminator, to compete in a zero-sum game~\cite{goodfellow_generative_2014}. TimeGAN~\cite{yoon_time-series_2019} is a centralized \gls*{gan} that exploits LSTM~\cite{hochreiter_long_1997} for generating time-series data. It consists of ``embedding'' (encodes data into a latent space) and ``recovery'' (reconstructs data from latent representations) functions that allow to preserve correlations and temporal dynamics of the original data. While centralized \glspl*{gan} have shown promising results in various applications, privacy concerns have led to the development of decentralized \glspl*{gan}, where the model is trained on edge devices without centralizing data~\cite{hardy_md-gan_2019}. In this work, we focus on deploying a decentralized \gls*{gan} model on individual vehicles to generate synthetic data that mimics the behavior of local data.

The \gls*{niid} nature of the data for each vehicle poses a challenge for training the decentralized \gls*{gan} model effectively. To address this challenge, the \gls*{f2u} approach ~\cite{yonetani_decentralized_2019} trains individual discriminators on local data and updates a generator to fool the most forgiving discriminator that deems generated samples as the most real. They also proposed \gls*{f2a}, which adaptively aggregates discriminators while emphasizing the most forgiving. Similarly, FeGAN~\cite{guerraoui_fegan_2020} fully distributes both the generator and the discriminator so that a private \gls*{gan} can be locally trained on each worker. These approaches help with preventing the vanishing gradients, mode collapse and learning divergence problems in a distributed setup.
%
In \cite{ekblom_effgan_2022}, %the authors examined the task of learning a data distribution when training data is decentralized among workers in an \gls*{fl} setup. 
authors proposed \gls*{effgan} as an ensemble of local expert generators that can learn the data distribution over all workers and mitigate client drift. It is able to train with a large number of local epochs and is more communication efficient than previous methods.

% \atB{Mention limitations of each approach, i.e., why we did not use them?}


\gls*{f2u} and \gls*{f2a} are beneficial for the deployment on resource-limited devices, as they necessitate having only the discriminator on the edge devices. Therefore, we adapt both of these architectures to work with time-series data. \gls*{f2u} and \gls*{f2a} train individual discriminators on each vehicle. In~\cite{yonetani_decentralized_2019}, the most forgiving discriminator is chosen to update the global generator. However, we found that the least forgiving discriminator gives a better performance. Moreover, we introduce an adaptive pooling layer in the discriminator to work with sequences of different  length, improving the generator's ability to re-create such sequences.
%
We also propose a novel normalization technique to further enhance the training process. More in details, the server determines the global minimum and maximum values for each feature, considering the lowest minimum and the highest maximum values received from all the workers. These global boundaries are then sent back to all workers for normalization of their local data. We experimentally show that this approach outperforms the normalization based on the known range of the specific sensors involved.

Our model can effectively learn from multiple \gls*{niid} time-series and converge to a solution that fools all discriminators. Experimental results show that our approach outperforms existing centralized methods, such as TimeGAN~\cite{yoon_time-series_2019}, for \gls*{niid} time-series in terms of the quality of the generated synthetic data. These findings have significant implications for the development of decentralized \glspl*{gan} in intelligent vehicles and other domains where privacy is a concern.
